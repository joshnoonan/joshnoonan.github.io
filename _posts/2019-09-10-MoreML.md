---
layout: post
title: More Machine Learning with Quarter Shooting Data
---

In my last post, I discussed the application of machine learning techniques to the quarter shooting data I had compiled in order to predict whether or not a team would make the playoffs. Based soley off of the z-score for a team's shooting performance in one quarter, we were able to achieve a roughly 66% success rate in predicting whether a team would make the playoffs. Upon further inspection of the places that the model was wrong, a large portion of the model's misses were on teams that had relatively strong offenses but miserable defenses. These teams might be scoring 110 a night, but they're giving up 120 to their opponents. With this in mind, I aimed to implement a parameter that could capture this, and settled upon net rating.

Net rating is a measure of a team's point differential per 100 possessions. It is calculated by subtracting a team's defensive rating (points allowed per 100 possessions) from their offensive rating (points scored per 100 possessions). Net rating itself is a fairly strong predictor of a team's playoff chances, as at a net rating of roughly 3.75 and higher, no team has ever failed to make the playoffs. Given this knowledge, I feel that a combination of net rating and z-scores from quarter shooting percentages would likely create an accurate model in predicting if a team will make the playoffs.

Without any further ado, let's get into constructing a k-nearest neighbors model to predict playoff chances.

As always, we'll start out with importing the necessary packages.
```python
import matplotlib.pyplot as plt
import pandas as pd
import pylab as pl
import numpy as np
%matplotlib inline
```

We'll then read in our data from a csv file aptly titled "everything.csv"
```python
qf = pd.read_csv("everything.csv")
qf.head(10)
```

We know need to add a column focusing specifically on whether or not a team made the playoffs, as the current format of the data has a column that represents how far a team advanced in the playoffs, but we want a simpler measure.
```python
qf['Playoff'] = [1 if x != 0 else 0 for x in qf['Playoff Points']]
qf.head(10)
```

We'll now eliminate columns that we don't need.
```python
bf = qf[['Team Name', 'Quarter' ,'Zscore' ,'Year', 'NRtg' ,'Playoff']]
bf.head()
```

Let's create a plot of all these data points, with a green dot representing a playoff team, and a red dot reprsenting a non-playoff (loterry) team.
```python
colors = ['green' if x > 0 else 'red' for x in qf['Playoff']]
plt.scatter(bf.Zscore, bf.NRtg,  s= 5, c=colors)
plt.xlabel("Z-Score")
plt.ylabel("Net Rating")
plt.show()
```
Which creates this figure!

![Beautiful Figure](https://raw.githubusercontent.com/joshnoonan/joshnoonan.github.io/master/images/greenred.png)

It's now time to fit a k-nearest neighbors algorithm to the data. We'll first split create our independent and dependent variables, and then split the data into training and test data sets.
```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
X1 = bf[['Zscore','NRtg']]
y1 = bf['Playoff'].values
X1_train, X1_test, y1_train, y1_test = train_test_split( X1, y1, test_size=0.2, random_state=4)
```

We'll know build the model for k = 10.
```pythonk = 10
#Train Model and Predict  
neigh1 = KNeighborsClassifier(n_neighbors = k).fit(X1_train,y1_train)
yhat1 = neigh1.predict(X1_test)
from sklearn import metrics
print("Train set Accuracy: ", metrics.accuracy_score(y1_train, neigh1.predict(X1_train)))
print("Test set Accuracy: ", metrics.accuracy_score(y1_test, yhat1))
```

Which produces a train set accuracy of 90.8% and a test set accuracy of 91.5%. Pretty good I would say! We'll now iterate through all k less than or equal to 30 to see which k generates the greatest accuracy.

```python
Ks = 30
mean_acc1 = np.zeros((Ks-1))
std_acc1 = np.zeros((Ks-1))
ConfustionMx = [];
for n in range(1,Ks):
    
    #Train Model and Predict  
    neigh1 = KNeighborsClassifier(n_neighbors = n).fit(X1_train,y1_train)
    yhat1=neigh1.predict(X1_test)
    mean_acc1[n-1] = metrics.accuracy_score(y1_test, yhat1)

    
    std_acc1[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])

mean_acc1
```

From this we can garner that the best accuracy was with 91.7% with k = 7. 0.2% is not a huge difference from the 91.5% accuracy at k = 10, but we can infer that the model is fairly accurate for most values of k. In my next post, I'm going to shift gears a little from what I have done and introduce a different machine learning technique. In it, I'll use machine learning to predict playoff probabilities for teams, instead of just predicting whether or not a team will make the playoffs. It's still the same general concept, but its product is a little more informative!
